{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeda8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')  # make sure Python knows where to look\n",
    "\n",
    "import os\n",
    "os.environ['DYLD_INSERT_LIBRARIES'] = ''\n",
    "if 'MallocStackLogging' in os.environ:\n",
    "    del os.environ['MallocStackLogging']\n",
    "\n",
    "import numpy as np\n",
    "# Fix deprecated np.int for NumPy 1.20+ (mlens library still uses deprecated aliases)\n",
    "if not hasattr(np, 'int'):\n",
    "    np.int = np.int64\n",
    "    np.float = np.float64\n",
    "    np.complex = np.complex128\n",
    "    np.object = np.object_\n",
    "    np.str = np.str_\n",
    "    np.long = np.int64\n",
    "    np.unicode = np.str_\n",
    "\n",
    "# Display full output in Jupyter\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from numpy import hstack, vstack\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import ml_training_functions as utils_training\n",
    "import file_handler_functions as utils_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08692b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: user 99.2 ms, sys: 121 ms, total: 220 ms\n",
      "Wall time: 277 ms\n",
      "622892 transactions loaded, containing 5515 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "# Load data from the 2018-07-25 to the 2018-08-14\n",
    "DIR_INPUT='./data/simulated-data-transformed/' \n",
    "\n",
    "BEGIN_DATE = \"2018-06-11\"\n",
    "END_DATE = \"2018-08-14\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df = utils_file.read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
    "\n",
    "START_DATE = \"2018-07-25\"\n",
    "delta_train = delta_delay = delta_test = delta_valid = delta_assessment = 7\n",
    "\n",
    "# Number of folds for the prequential validation\n",
    "n_folds = 4\n",
    "\n",
    "start_date_training = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "start_date_test = start_date_training+datetime.timedelta(days=delta_train+delta_delay)\n",
    "\n",
    "start_date_training_for_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
    "start_date_training_for_test = start_date_training+datetime.timedelta(days=(n_folds-1)*delta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e0d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature = \"TX_FRAUD\"\n",
    "\n",
    "input_features = ['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
    "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
    "       'TERMINAL_ID_RISK_30DAY_WINDOW']\n",
    "\n",
    "# Only keep columns that are needed as argument to the custom scoring function\n",
    "# (in order to reduce the serialization time of transaction dataset)\n",
    "transactions_df_scorer = transactions_df[['CUSTOMER_ID', 'TX_FRAUD','TX_TIME_DAYS']]\n",
    "\n",
    "# Split dataset for Cross validation\n",
    "prequential_split_indices=utils_training.prequentialSplit(transactions_df,\n",
    "                                               start_date_training=start_date_training, \n",
    "                                               n_folds=n_folds, \n",
    "                                               delta_train=delta_train, \n",
    "                                               delta_delay=delta_delay, \n",
    "                                               delta_assessment=delta_assessment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_models_tuning():\n",
    "    return {\n",
    "        'XGBClassifier': XGBClassifier(\n",
    "            learning_rate = 0.3,\n",
    "            max_depth = 3,\n",
    "            n_estimators = 100,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            scale_pos_weight = 10\n",
    "        ),\n",
    "        'LGBMClassifier': LGBMClassifier(\n",
    "            learning_rate = 0.1,\n",
    "            max_depth = 3,\n",
    "            n_estimators = 100,\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            scale_pos_weight=1, \n",
    "            verbosity=-1\n",
    "        ),\n",
    "        'CatBoostClassifier': CatBoostClassifier(\n",
    "            depth = 4,\n",
    "            iterations = 500,\n",
    "            learning_rate = 0.05,\n",
    "            random_state = 0,\n",
    "            scale_pos_weight = 5,\n",
    "            # prefer class_weights if heavy imbalance; we'll rely on default + tuned params\n",
    "            # class_weights can be added if desired, e.g. class_weights=[1, 99]\n",
    "            verbose = 0\n",
    "        ),\n",
    "        'HistGradientBoostingClassifier': HistGradientBoostingClassifier(\n",
    "            max_iter = 100,\n",
    "            learning_rate = 0.05,\n",
    "            max_depth = 5,\n",
    "            random_state = 0,\n",
    "            class_weight = 'balanced'\n",
    "        ),\n",
    "        'BalancedRandomForestClassifier': BalancedRandomForestClassifier(\n",
    "            max_depth = 20,\n",
    "            n_estimators = 100,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            sampling_strategy = 0.05\n",
    "        ),\n",
    "        'BalancedBaggingClassifier': BalancedBaggingClassifier(\n",
    "            bootstrap = True,\n",
    "            estimator = DecisionTreeClassifier(max_depth=20, random_state=0),\n",
    "            n_estimators = 100,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            sampler = RandomUnderSampler(),\n",
    "            sampling_strategy = 0.1\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            C=0.1,\n",
    "            random_state=0,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "def get_base_models():\n",
    "    return {\n",
    "        'XGBClassifier': XGBClassifier(\n",
    "            learning_rate = 0.1,\n",
    "            max_depth = 3,\n",
    "            n_estimators = 50,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            scale_pos_weight = 10\n",
    "        ),\n",
    "        'LGBMClassifier': LGBMClassifier(\n",
    "            learning_rate = 0.1,\n",
    "            max_depth = 3,\n",
    "            n_estimators = 50,\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            scale_pos_weight=1\n",
    "        ),\n",
    "        'CatBoostClassifier': CatBoostClassifier(\n",
    "            depth = 4,\n",
    "            iterations = 500,\n",
    "            learning_rate = 0.05,\n",
    "            random_state = 0,\n",
    "            class_weights=[1, 99],\n",
    "            verbose = 0\n",
    "        ),\n",
    "        'HistGradientBoostingClassifier': HistGradientBoostingClassifier(\n",
    "            max_iter = 100,\n",
    "            learning_rate = 0.05,\n",
    "            max_depth = 5,\n",
    "            random_state = 0,\n",
    "            class_weight = 'balanced'\n",
    "        ),\n",
    "        'BalancedRandomForestClassifier': BalancedRandomForestClassifier(\n",
    "            max_depth = 50,\n",
    "            n_estimators = 100,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            sampling_strategy = 0.1\n",
    "        ),\n",
    "        'BalancedBaggingClassifier': BalancedBaggingClassifier(\n",
    "            bootstrap = True,\n",
    "            estimator = DecisionTreeClassifier(max_depth=20, random_state=0),\n",
    "            n_estimators = 100,\n",
    "            n_jobs = -1,\n",
    "            random_state = 0,\n",
    "            sampler = RandomUnderSampler(),\n",
    "            sampling_strategy = 0.1\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            C=0.1,\n",
    "            random_state=0,\n",
    "            solver='liblinear'\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# for this function, the transactions_df_scorer must contain CUSTOMER_ID and TX_TIME_DAYS for the full df; X_index are the indices for the current test fold.\n",
    "def card_precision_top_k_wrapper(probs, X_index, transactions_df_scorer, k=100):\n",
    "    preds_df = transactions_df_scorer.loc[X_index].copy()\n",
    "    preds_df['predictions'] = probs\n",
    "    nb, per_day_list, mean_cp = utils_training.card_precision_top_k(preds_df, k)\n",
    "    return mean_cp\n",
    "\n",
    "# out-of-fold builder (prequential-friendly)\n",
    "def get_out_of_fold_predictions_no_sampling(transactions_df, prequential_split_indices, base_models_dict,\n",
    "                                           input_features, output_feature, transactions_df_scorer=None):\n",
    "    meta_rows = []\n",
    "    meta_labels = []\n",
    "    meta_indices = []\n",
    "    model_names = list(base_models_dict.keys())\n",
    "\n",
    "    # iterate prequential folds\n",
    "    for fold_i, (train_ix, test_ix) in enumerate(prequential_split_indices):\n",
    "        # get train/test slices (no sampling)\n",
    "        train_df = transactions_df.iloc[train_ix]\n",
    "        test_df = transactions_df.iloc[test_ix]\n",
    "        X_train, y_train, X_test, y_test = get_train_test_features(train_df, test_df, input_features, output_feature)\n",
    "\n",
    "        # scale: fit scaler on training fold only, transform train/test\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # collect base model predictions\n",
    "        fold_preds = []\n",
    "\n",
    "        for name in model_names:\n",
    "            # initailized a new model instance on different fold\n",
    "            model = clone(base_models_dict[name]) \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            probs = get_predict_proba(model, X_test_scaled)\n",
    "\n",
    "            fold_preds.append(probs.reshape(-1, 1))\n",
    "\n",
    "        # stack column-wise to shape (n_test_rows, n_models)\n",
    "        fold_meta_X = np.hstack(fold_preds)\n",
    "\n",
    "        meta_rows.append(fold_meta_X)\n",
    "        meta_labels.append(y_test)\n",
    "        \n",
    "        # keep original indices for evaluation (card precision)\n",
    "        meta_indices.append(test_df.index)\n",
    "\n",
    "    # vertically stack folds\n",
    "    meta_X = np.vstack(meta_rows)\n",
    "    meta_y = np.concatenate(meta_labels)\n",
    "    meta_index = np.concatenate([np.array(idx) for idx in meta_indices])\n",
    "\n",
    "    return meta_X, meta_y, meta_index, model_names\n",
    "\n",
    "# Fit base models on full training data (no sampling)\n",
    "def fit_base_models_full(X_train_df, y_train_series, base_models_dict):\n",
    "    fitted_models = {}\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df.values)   # we will also store this scaler for test transforms if needed\n",
    "    for name, model in base_models_dict.items():\n",
    "        m = clone(model)\n",
    "        m.fit(X_train_scaled, y_train_series.values)\n",
    "        fitted_models[name] = m\n",
    "    return fitted_models, scaler\n",
    "\n",
    "# Fit meta model (LogisticRegression) on meta features\n",
    "def fit_meta_model(base_models, meta_X, meta_y):\n",
    "    meta_model = base_models['LogisticRegression']\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "    return meta_model\n",
    "\n",
    "# Super-learner prediction helper for producing probability predictions on test set\n",
    "def super_learner_predict_proba(models_fitted_dict, meta_model, X_test_df, scaler=None):\n",
    "    # transform X_test appropriately (if scaler provided)\n",
    "    if scaler is not None:\n",
    "        X_test_scaled = scaler.transform(X_test_df.values)\n",
    "    else:\n",
    "        X_test_scaled = X_test_df.values\n",
    "\n",
    "    probs_list = []\n",
    "    for _, model in models_fitted_dict.items():\n",
    "        probs = get_predict_proba(model, X_test_scaled)\n",
    "        probs_list.append(probs)\n",
    "        \n",
    "    meta_X_test = np.hstack(probs_list)\n",
    "    \n",
    "    # meta_model should support predict_proba\n",
    "    if hasattr(meta_model, \"predict_proba\"):\n",
    "        return meta_model.predict_proba(meta_X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback\n",
    "        return meta_model.predict(meta_X_test)\n",
    "\n",
    "def get_train_test_features(train_df, test_df, input_features, output_feature):\n",
    "    return (train_df[input_features].values, \n",
    "            train_df[output_feature].values,\n",
    "            test_df[input_features].values,\n",
    "            test_df[output_feature].values)\n",
    "\n",
    "def get_predict_proba(model, X_scaled):\n",
    "    if hasattr(model, \"predict_proba\"): # some models might not implement predict_proba\n",
    "        return model.predict_proba(X_scaled)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):  # if model implement decision_function -> convert to probabilities via logistic/sigmoid\n",
    "        dec = model.decision_function(X_scaled)\n",
    "        return 1.0 / (1.0 + np.exp(-dec))  # sigmoid to convert to probability-like\n",
    "    else: # last resort: use predict (not recommended)\n",
    "        return model.predict(X_scaled).astype(float)\n",
    "\n",
    "def get_performance_metrics(df, y, probs, transactions_df_scorer):\n",
    "    auc = roc_auc_score(y, probs)\n",
    "    ap  = average_precision_score(y, probs)\n",
    "    cp = None\n",
    "    if transactions_df_scorer is not None:\n",
    "        cp = card_precision_top_k_wrapper(probs, df.index, transactions_df_scorer, k=100)\n",
    "        \n",
    "    return auc, ap, cp\n",
    "    \n",
    "def evaluate_base_models_per_fold(base_models_dict, train_df, test_df,\n",
    "                                  input_features, output_feature,\n",
    "                                  transactions_df_scorer=None):\n",
    "    X_train, y_train, X_test, y_test = get_train_test_features(train_df, test_df, input_features, output_feature)\n",
    "\n",
    "    # scale using train only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    fold_results = {}\n",
    "\n",
    "    for name, model in base_models_dict.items():\n",
    "        m = clone(model)\n",
    "        m.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predicting train set and test set\n",
    "        train_probs = get_predict_proba(m, X_train_scaled)\n",
    "        test_probs = get_predict_proba(m, X_test_scaled)\n",
    "        \n",
    "        train_auc, train_ap, train_cp = get_performance_metrics(train_df, y_train, train_probs, transactions_df_scorer)\n",
    "        test_auc, test_ap, test_cp = get_performance_metrics(test_df, y_test, test_probs, transactions_df_scorer)\n",
    "\n",
    "        train_performance_detail = {'n_train': len(train_df), 'auc': train_auc, 'ap': train_ap, 'cp100': train_cp}\n",
    "        test_performance_detail = {'n_train': len(test_df), 'auc': test_auc, 'ap': test_ap, 'cp100': test_cp}\n",
    "        fold_results[name] = {'train': train_performance_detail, 'test': test_performance_detail}\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "def evaluate_super_learner_per_fold(fitted_base_models, meta_model,\n",
    "                                    train_df, test_df,\n",
    "                                    input_features, output_feature,\n",
    "                                    scaler, transactions_df_scorer=None):\n",
    "    X_train, y_train, X_test, y_test = get_train_test_features(train_df, test_df, input_features, output_feature)\n",
    "\n",
    "    # scale test set with global scaler\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # build meta features for train\n",
    "    train_meta = []\n",
    "    for m in fitted_base_models.values():\n",
    "        train_probs = get_predict_proba(m, X_train_scaled)\n",
    "        train_meta.append(train_probs.reshape(-1, 1))\n",
    "    X_train_meta = np.hstack(train_meta)\n",
    "\n",
    "    # build meta features for test\n",
    "    test_meta = []\n",
    "    for m in fitted_base_models.values():\n",
    "        test_probs = get_predict_proba(m, X_test_scaled)\n",
    "        test_meta.append(test_probs.reshape(-1, 1))\n",
    "    X_test_meta = np.hstack(test_meta)\n",
    "\n",
    "    # predict using meta model\n",
    "    train_probs = meta_model.predict_proba(X_train_meta)[:, 1]\n",
    "    test_probs  = meta_model.predict_proba(X_test_meta)[:, 1]\n",
    "    \n",
    "    train_auc, train_ap, train_cp = get_performance_metrics(train_df, y_train, train_probs, transactions_df_scorer)\n",
    "    test_auc, test_ap, test_cp = get_performance_metrics(test_df, y_test, test_probs, transactions_df_scorer)\n",
    "\n",
    "    train_performance_detail = {'n_train': len(train_df), 'auc': train_auc, 'ap': train_ap, 'cp100': train_cp}\n",
    "    test_performance_detail = {'n_train': len(test_df), 'auc': test_auc, 'ap': test_ap, 'cp100': test_cp}\n",
    "    return {'train': train_performance_detail, 'test': test_performance_detail}\n",
    "\n",
    "def summarize_performance_across_folds(per_fold_results):\n",
    "    train_auc = []\n",
    "    train_ap = []\n",
    "    train_cp = []\n",
    "    test_auc = []\n",
    "    test_ap = []\n",
    "    test_cp = []\n",
    "\n",
    "    for item in per_fold_results:\n",
    "        r_train = item['results']['train']\n",
    "        r_test  = item['results']['test']\n",
    "\n",
    "        train_auc.append(r_train['auc'])\n",
    "        train_ap.append(r_train['ap'])\n",
    "        train_cp.append(r_train['cp100'])\n",
    "\n",
    "        test_auc.append(r_test['auc'])\n",
    "        test_ap.append(r_test['ap'])\n",
    "        test_cp.append(r_test['cp100'])\n",
    "\n",
    "    return {\n",
    "        'train_auc_mean': f'{np.mean(train_auc):.4f}',\n",
    "        'train_ap_mean': f'{np.mean(train_ap):.4f}',\n",
    "        'train_cp100_mean': f'{np.mean(train_cp):.4f}',\n",
    "        'test_auc_mean': f'{np.mean(test_auc):.4f}',\n",
    "        'test_ap_mean': f'{np.mean(test_ap):.4f}',\n",
    "        'test_cp100_mean': f'{np.mean(test_cp):.4f}',\n",
    "    }\n",
    "\n",
    "# Full pipeline runner (ties everything together)\n",
    "def run_super_learner(base_models, transactions_df, prequential_split_indices,\n",
    "                              input_features, output_feature,\n",
    "                              transactions_df_scorer=None, verbose=True):\n",
    "\n",
    "    # -------------------- OOF meta features --------------------\n",
    "    meta_X, meta_y, meta_index, model_names = get_out_of_fold_predictions_no_sampling(\n",
    "        transactions_df, prequential_split_indices, base_models,\n",
    "        input_features, output_feature, transactions_df_scorer\n",
    "    )\n",
    "    meta_model = fit_meta_model(base_models, meta_X, meta_y)\n",
    "\n",
    "    # -------------------- fit base models on first window --------------------\n",
    "    first_train_ix, _ = prequential_split_indices[0]\n",
    "    train_df_full = transactions_df.iloc[first_train_ix]\n",
    "    X_train_full = train_df_full[input_features]\n",
    "    y_train_full = train_df_full[output_feature]\n",
    "\n",
    "    fitted_base_models, global_scaler = fit_base_models_full(\n",
    "        X_train_full, y_train_full, base_models\n",
    "    )\n",
    "\n",
    "    # -------------------- Evaluate per fold --------------------\n",
    "    super_learner_performance = []\n",
    "    base_model_performance = {name: [] for name in base_models.keys()}\n",
    "\n",
    "    for fold_i, (train_ix, test_ix) in enumerate(prequential_split_indices):\n",
    "        train_df = transactions_df.iloc[train_ix]\n",
    "        test_df  = transactions_df.iloc[test_ix]\n",
    "\n",
    "        # ---- base models per fold ----\n",
    "        base_fold_result = evaluate_base_models_per_fold(\n",
    "            base_models, train_df, test_df,\n",
    "            input_features, output_feature,\n",
    "            transactions_df_scorer\n",
    "        )\n",
    "        for model_name, metrics in base_fold_result.items():\n",
    "            base_model_performance[model_name].append({'fold': fold_i, 'results': metrics})\n",
    "\n",
    "        # ---- super learner per fold ----\n",
    "        super_learner_fold_result = evaluate_super_learner_per_fold(\n",
    "            fitted_base_models, meta_model,\n",
    "            train_df, test_df,\n",
    "            input_features, output_feature,\n",
    "            global_scaler, transactions_df_scorer\n",
    "        )\n",
    "        super_learner_performance.append({'fold': fold_i, 'results': super_learner_fold_result })\n",
    "\n",
    "    return {\n",
    "        'meta_model': meta_model,\n",
    "        'fitted_base_models': fitted_base_models,\n",
    "        'global_scaler': global_scaler,\n",
    "        'base_model_performance': base_model_performance,\n",
    "        'super_learner_performance': super_learner_performance\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = get_base_models()\n",
    "\n",
    "preformance_results = run_super_learner(base_models, transactions_df, prequential_split_indices,\n",
    "                            input_features, output_feature,\n",
    "                            transactions_df_scorer=transactions_df_scorer,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626ed291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_auc_mean</th>\n",
       "      <th>train_ap_mean</th>\n",
       "      <th>train_cp100_mean</th>\n",
       "      <th>test_auc_mean</th>\n",
       "      <th>test_ap_mean</th>\n",
       "      <th>test_cp100_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SuperLearner</th>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.4521</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>0.7593</td>\n",
       "      <td>0.3229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>0.6318</td>\n",
       "      <td>0.2921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.9469</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>0.4311</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>0.6488</td>\n",
       "      <td>0.2896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.8816</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.2921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingClassifier</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.2754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5464</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.2914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedBaggingClassifier</th>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.6913</td>\n",
       "      <td>0.2929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.5941</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.5419</td>\n",
       "      <td>0.2825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               train_auc_mean train_ap_mean train_cp100_mean  \\\n",
       "SuperLearner                           0.9256        0.8018           0.4521   \n",
       "XGBClassifier                          0.9343        0.7378           0.4264   \n",
       "LGBMClassifier                         0.9469        0.7774           0.4311   \n",
       "CatBoostClassifier                     0.9945        0.7950           0.4636   \n",
       "HistGradientBoostingClassifier         0.9724        0.7660           0.4354   \n",
       "BalancedRandomForestClassifier         1.0000        1.0000           0.5464   \n",
       "BalancedBaggingClassifier              0.9994        0.9489           0.5350   \n",
       "LogisticRegression                     0.8996        0.5941           0.3982   \n",
       "\n",
       "                               test_auc_mean test_ap_mean test_cp100_mean  \n",
       "SuperLearner                          0.9078       0.7593          0.3229  \n",
       "XGBClassifier                         0.8719       0.6318          0.2921  \n",
       "LGBMClassifier                        0.8794       0.6488          0.2896  \n",
       "CatBoostClassifier                    0.8816       0.6065          0.2921  \n",
       "HistGradientBoostingClassifier        0.8541       0.6004          0.2754  \n",
       "BalancedRandomForestClassifier        0.8811       0.6549          0.2914  \n",
       "BalancedBaggingClassifier             0.8804       0.6913          0.2929  \n",
       "LogisticRegression                    0.8704       0.5419          0.2825  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "res[\"SuperLearner\"] = summarize_performance_across_folds(preformance_results['super_learner_performance'])\n",
    "for name, performance_result in preformance_results['base_model_performance'].items():\n",
    "    res[name] = summarize_performance_across_folds(performance_result)\n",
    "    \n",
    "pd.DataFrame.from_dict(res, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f7534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = get_base_models_tuning()\n",
    "\n",
    "preformance_results_tuning = run_super_learner(base_models, transactions_df, prequential_split_indices,\n",
    "                            input_features, output_feature,\n",
    "                            transactions_df_scorer=transactions_df_scorer,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c5746a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_auc_mean</th>\n",
       "      <th>train_ap_mean</th>\n",
       "      <th>train_cp100_mean</th>\n",
       "      <th>test_auc_mean</th>\n",
       "      <th>test_ap_mean</th>\n",
       "      <th>test_cp100_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SuperLearner</th>\n",
       "      <td>0.9244</td>\n",
       "      <td>0.7989</td>\n",
       "      <td>0.4475</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.3214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>0.8798</td>\n",
       "      <td>0.6445</td>\n",
       "      <td>0.2864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>0.6535</td>\n",
       "      <td>0.2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.8322</td>\n",
       "      <td>0.4454</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>0.6721</td>\n",
       "      <td>0.2943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingClassifier</th>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.2754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier</th>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>0.2904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedBaggingClassifier</th>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.6913</td>\n",
       "      <td>0.2929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.8996</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.5418</td>\n",
       "      <td>0.2825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               train_auc_mean train_ap_mean train_cp100_mean  \\\n",
       "SuperLearner                           0.9244        0.7989           0.4475   \n",
       "XGBClassifier                          0.9934        0.8701           0.4746   \n",
       "LGBMClassifier                         0.9660        0.8100           0.4379   \n",
       "CatBoostClassifier                     0.9710        0.8322           0.4454   \n",
       "HistGradientBoostingClassifier         0.9724        0.7660           0.4354   \n",
       "BalancedRandomForestClassifier         0.9991        0.9460           0.5189   \n",
       "BalancedBaggingClassifier              0.9994        0.9489           0.5350   \n",
       "LogisticRegression                     0.8996        0.5940           0.3982   \n",
       "\n",
       "                               test_auc_mean test_ap_mean test_cp100_mean  \n",
       "SuperLearner                          0.9082       0.7538          0.3214  \n",
       "XGBClassifier                         0.8798       0.6445          0.2864  \n",
       "LGBMClassifier                        0.8856       0.6535          0.2893  \n",
       "CatBoostClassifier                    0.8860       0.6721          0.2943  \n",
       "HistGradientBoostingClassifier        0.8541       0.6004          0.2754  \n",
       "BalancedRandomForestClassifier        0.8844       0.6673          0.2904  \n",
       "BalancedBaggingClassifier             0.8804       0.6913          0.2929  \n",
       "LogisticRegression                    0.8704       0.5418          0.2825  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "res[\"SuperLearner\"] = summarize_performance_across_folds(preformance_results_tuning['super_learner_performance'])\n",
    "for name, performance_result in preformance_results_tuning['base_model_performance'].items():\n",
    "    res[name] = summarize_performance_across_folds(performance_result)\n",
    "    \n",
    "pd.DataFrame.from_dict(res, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13a2db",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b95d605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/super_learner_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "FOLDER_PATH = \"./models\"\n",
    "FILE_PATH = FOLDER_PATH+\"/super_learner_model.pkl\"\n",
    "\n",
    "os.makedirs(FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "joblib.dump({\n",
    "    'base_models': preformance_results_tuning['fitted_base_models'],\n",
    "    'meta_model': preformance_results_tuning['meta_model'],\n",
    "    'scaler': preformance_results_tuning['global_scaler'],\n",
    "    'input_features': input_features,\n",
    "}, FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
